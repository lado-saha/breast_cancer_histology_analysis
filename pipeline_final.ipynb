{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Breast Cancer Histology: Full Pipeline with Advanced Segmentation\n",
        "\n",
        "This notebook implements the complete workflow using an SVM classifier and an advanced nuclei segmentation strategy:\n",
        "1. Finds all relevant image paths and extracts patient/slide IDs.\n",
        "2. Splits patient/slide IDs into Training and Testing sets (patient-level split).\n",
        "3. Extracts features ONLY from the Training image set using advanced segmentation.\n",
        "4. Cleans, preprocesses (scaling), and trains an SVM model on the Training features.\n",
        "5. Extracts features ONLY from the Testing image set using advanced segmentation.\n",
        "6. Cleans and preprocesses (using the scaler fitted on Training data) the Testing features.\n",
        "7. Evaluates the trained SVM model on the unseen Testing features.\n",
        "\n",
        "**Advanced Segmentation Strategy:**\n",
        "*   Color Deconvolution (Hematoxylin channel isolation)\n",
        "*   Adaptive Thresholding (Otsu) or Manual Thresholding\n",
        "*   Morphological Operations (Opening, Closing)\n",
        "*   Watershed Segmentation (Optional, for separating nuclei)\n",
        "*   Contour Filtering (Area, Circularity, per magnification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "import pathlib\n",
        "import json # For potentially loading/displaying config\n",
        "\n",
        "from tqdm.notebook import tqdm # Use tqdm.notebook for Jupyter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "\n",
        "# Scikit-image for advanced segmentation\n",
        "from skimage import color, exposure\n",
        "# from scipy import ndimage # Not strictly needed if cv2.connectedComponents is used\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    precision_recall_fscore_support\n",
        ")\n",
        "from sklearn.exceptions import NotFittedError\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Path to Dataset ---\n",
        "notebook_launch_dir = pathlib.Path.cwd()\n",
        "ROOT_DATA_DIR = notebook_launch_dir / 'raw'\n",
        "\n",
        "# --- Modeling Parameters ---\n",
        "TEST_SET_SIZE = 0.25  # Proportion of unique patient/slide IDs for the test set\n",
        "RANDOM_STATE = 42\n",
        "NAN_HANDLING_STRATEGY = 'drop' # 'drop' or 'impute_mean'\n",
        "\n",
        "# --- Feature Names ---\n",
        "FEATURE_NAMES_BASE = ['radius', 'texture', 'perimeter', 'area', 'smoothness', \n",
        "                      'compactness', 'concavity', 'concave_points', 'symmetry', \n",
        "                      'fractal_dimension']\n",
        "# For Mean, Standard Error (SE), and Max of worst/largest nuclei features\n",
        "FEATURE_COLUMNS = [f'{name}{suffix}' for name in FEATURE_NAMES_BASE for suffix in ['_mean', '_se', '_max']]\n",
        "\n",
        "# --- YOUR TUNED SEGMENTATION CONFIGURATION ---\n",
        "# IMPORTANT: Replace this example with the actual SEGMENTATION_CONFIG \n",
        "# dictionary that you fine-tuned using the Gradio UI or manual iteration.\n",
        "SEGMENTATION_CONFIG = {\n",
        "    'contrast_stretch': False, \n",
        "    'contrast_percentiles_low': 2,\n",
        "    'contrast_percentiles_high': 98,\n",
        "    'threshold_method': 'otsu', # 'otsu' or 'manual'\n",
        "    'manual_threshold_value': 100, # Used if threshold_method is 'manual'\n",
        "    'morph_open_kernel_size': 3, # Kernel size (int, will be (size,size))\n",
        "    'morph_open_iterations': 1,      \n",
        "    'morph_close_kernel_size': 3, # Kernel size (int)\n",
        "    'morph_close_iterations': 1,      \n",
        "    'use_watershed': True,           \n",
        "    'dist_transform_thresh_ratio': 0.3, # Global fallback (primarily for reference, mag-specific is used)\n",
        "    'contour_filters_by_magnification': {\n",
        "        '40X':  {'min_area': 10,   'max_area': 300,  'min_circularity': 0.3, 'dist_transform_thresh_ratio': 0.2},\n",
        "        '100X': {'min_area': 40,  'max_area': 1000, 'min_circularity': 0.3, 'dist_transform_thresh_ratio': 0.3},\n",
        "        '200X': {'min_area': 80, 'max_area': 3000, 'min_circularity': 0.3, 'dist_transform_thresh_ratio': 0.5},\n",
        "        '400X': {'min_area': 200, 'max_area': 7000, 'min_circularity': 0.3, 'dist_transform_thresh_ratio': 0.6}\n",
        "        # ^^^ THESE ARE EXAMPLES - USE YOUR TUNED VALUES ^^^ \n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Helper Functions (Segmentation, Feature Calculation, Evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_hematoxylin_channel(image_rgb):\n",
        "    \"\"\"Extracts the Hematoxylin channel using scikit-image's color deconvolution.\"\"\"\n",
        "    # Add a small epsilon to prevent division by zero or log of zero if image has pure black pixels\n",
        "    image_rgb_safe = np.clip(image_rgb, 1, 255) \n",
        "    ihc_hed = color.rgb2hed(image_rgb_safe)\n",
        "    h_channel = ihc_hed[:, :, 0]\n",
        "    # Normalize to 0-255, nuclei should be bright (higher values)\n",
        "    h_channel_norm = cv2.normalize(h_channel, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "    return h_channel_norm\n",
        "\n",
        "def calculate_circularity(contour):\n",
        "    \"\"\"Calculates the circularity of a contour.\"\"\"\n",
        "    perimeter = cv2.arcLength(contour, True)\n",
        "    area = cv2.contourArea(contour)\n",
        "    if perimeter == 0 or area == 0:\n",
        "        return 0\n",
        "    return (4 * np.pi * area) / (perimeter ** 2)\n",
        "\n",
        "def segment_nuclei_pipeline(image_bgr, magnification, config):\n",
        "    \"\"\"Advanced nuclei segmentation for pipeline use. Returns only filtered contours.\"\"\"\n",
        "    if image_bgr is None: return []\n",
        "\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    h_channel = get_hematoxylin_channel(image_rgb)\n",
        "    h_channel_processed = h_channel.copy()\n",
        "\n",
        "    if config.get('contrast_stretch', False):\n",
        "        p_low = config['contrast_percentiles_low']\n",
        "        p_high = config['contrast_percentiles_high']\n",
        "        if 0 <= p_low < p_high <= 100:\n",
        "            p_low_val, p_high_val = np.percentile(h_channel_processed, (p_low, p_high))\n",
        "            if p_low_val < p_high_val: # Check if percentiles are distinct\n",
        "                h_channel_processed = exposure.rescale_intensity(h_channel_processed, in_range=(p_low_val, p_high_val))\n",
        "                h_channel_processed = cv2.normalize(h_channel_processed, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "\n",
        "    if config['threshold_method'] == 'otsu':\n",
        "        _, binary_mask = cv2.threshold(h_channel_processed, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    else: # manual\n",
        "        _, binary_mask = cv2.threshold(h_channel_processed, config['manual_threshold_value'], 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    open_k_size = config['morph_open_kernel_size']\n",
        "    if open_k_size % 2 == 0: open_k_size +=1 # Ensure odd kernel size\n",
        "    close_k_size = config['morph_close_kernel_size']\n",
        "    if close_k_size % 2 == 0: close_k_size +=1 # Ensure odd kernel size\n",
        "    \n",
        "    kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (open_k_size, open_k_size))\n",
        "    opened_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel_open, iterations=config['morph_open_iterations'])\n",
        "    \n",
        "    kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (close_k_size, close_k_size))\n",
        "    cleaned_mask = cv2.morphologyEx(opened_mask, cv2.MORPH_CLOSE, kernel_close, iterations=config['morph_close_iterations'])\n",
        "\n",
        "    final_contours_list = []\n",
        "    # Get magnification-specific filters, fallback to '200X' if current mag not found (should not happen with good config)\n",
        "    mag_specific_filters = config['contour_filters_by_magnification'].get(magnification, \n",
        "                                                                        config['contour_filters_by_magnification'].get('200X'))\n",
        "\n",
        "    if config.get('use_watershed', True):\n",
        "        sure_bg_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3)) \n",
        "        sure_bg = cv2.dilate(cleaned_mask, sure_bg_kernel, iterations=3)\n",
        "        dist_transform = cv2.distanceTransform(cleaned_mask, cv2.DIST_L2, 5)\n",
        "        \n",
        "        default_global_dt_ratio = config.get('dist_transform_thresh_ratio', 0.3) # Global fallback from main config\n",
        "        current_dist_thresh_ratio = mag_specific_filters.get('dist_transform_thresh_ratio', default_global_dt_ratio)\n",
        "        \n",
        "        _, sure_fg = cv2.threshold(dist_transform, current_dist_thresh_ratio * dist_transform.max(), 255, 0)\n",
        "        sure_fg = np.uint8(sure_fg)\n",
        "        unknown = cv2.subtract(sure_bg, sure_fg)\n",
        "        _, markers = cv2.connectedComponents(sure_fg)\n",
        "        markers = markers + 1 # So background is 1, not 0\n",
        "        markers[unknown == 255] = 0 # Mark unknown region\n",
        "        \n",
        "        try:\n",
        "            markers_copy = markers.copy() # watershed modifies markers in-place\n",
        "            watershed_input_img = image_rgb.copy() # Use a copy for watershed if original image_rgb is needed later\n",
        "            cv2.watershed(watershed_input_img, markers_copy)\n",
        "            \n",
        "            unique_marker_values = np.unique(markers_copy) # Use the modified markers_copy\n",
        "            for marker_val in unique_marker_values:\n",
        "                if marker_val <= 1:  # Skip background (1) and watershed boundary (-1)\n",
        "                    continue\n",
        "                nucleus_mask_ws = np.zeros(cleaned_mask.shape, dtype=np.uint8)\n",
        "                nucleus_mask_ws[markers_copy == marker_val] = 255\n",
        "                contours_ws, _ = cv2.findContours(nucleus_mask_ws, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                if contours_ws: final_contours_list.extend(contours_ws)\n",
        "        except cv2.error as e:\n",
        "            # print(f\"Pipeline Watershed failed (mag: {magnification}, file: {os.path.basename(image_bgr.name if hasattr(image_bgr, 'name') else 'N/A')}): {e}. Falling back.\")\n",
        "            contours_fb, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            final_contours_list = contours_fb\n",
        "    else: # No watershed\n",
        "        contours_no_ws, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        final_contours_list = contours_no_ws\n",
        "\n",
        "    min_area = mag_specific_filters['min_area']\n",
        "    max_area = mag_specific_filters['max_area']\n",
        "    min_circ = mag_specific_filters['min_circularity']\n",
        "    \n",
        "    filtered_contours = []\n",
        "    for cnt in final_contours_list:\n",
        "        area = cv2.contourArea(cnt)\n",
        "        if min_area < area < max_area:\n",
        "            circularity = calculate_circularity(cnt)\n",
        "            if circularity >= min_circ:\n",
        "                filtered_contours.append(cnt)\n",
        "    return filtered_contours\n",
        "\n",
        "\n",
        "def calculate_contour_features(contour, gray_image):\n",
        "    \"\"\"Calculates a dictionary of features for a single contour.\"\"\"\n",
        "    features = {}\n",
        "    try:\n",
        "        moments = cv2.moments(contour)\n",
        "        area = cv2.contourArea(contour)\n",
        "        if area <= 0: return None # Skip invalid contours\n",
        "\n",
        "        perimeter = cv2.arcLength(contour, True)\n",
        "        equivalent_diameter = np.sqrt(4 * area / np.pi)\n",
        "        features['radius'] = equivalent_diameter / 2.0\n",
        "\n",
        "        # Texture (standard deviation of gray levels within the contour)\n",
        "        mask = np.zeros(gray_image.shape, dtype=np.uint8)\n",
        "        cv2.drawContours(mask, [contour], -1, color=255, thickness=cv2.FILLED)\n",
        "        mean_val, stddev_val = cv2.meanStdDev(gray_image, mask=mask)\n",
        "        features['texture'] = stddev_val[0][0] if stddev_val is not None and stddev_val.size > 0 else 0.0\n",
        "\n",
        "        features['perimeter'] = perimeter\n",
        "        features['area'] = area\n",
        "\n",
        "        # Smoothness (std of distances from center to contour points)\n",
        "        if moments['m00'] > 0 and len(contour) > 1:\n",
        "            cx = int(moments['m10'] / moments['m00'])\n",
        "            cy = int(moments['m01'] / moments['m00'])\n",
        "            center = np.array([cx, cy])\n",
        "            contour_points = contour.reshape(-1, 2)\n",
        "            distances = np.sqrt(np.sum((contour_points - center)**2, axis=1))\n",
        "            features['smoothness'] = np.std(distances) if distances.size > 0 else 0.0\n",
        "        else:\n",
        "            features['smoothness'] = 0.0\n",
        "\n",
        "        # Compactness\n",
        "        features['compactness'] = (perimeter ** 2) / area if area > 0 else 0.0\n",
        "\n",
        "        # Concavity (related to solidity)\n",
        "        hull = cv2.convexHull(contour)\n",
        "        hull_area = cv2.contourArea(hull)\n",
        "        if hull_area > 0:\n",
        "            features['concavity'] = 1.0 - (area / hull_area) # (hull_area - area) / hull_area\n",
        "        else:\n",
        "            features['concavity'] = 0.0\n",
        "\n",
        "        # Concave points (number of significant concavities)\n",
        "        try:\n",
        "            if len(contour) > 3: # Need at least 4 points for convexityDefects\n",
        "                hull_indices = cv2.convexHull(contour, returnPoints=False) # Get indices for defects\n",
        "                if hull_indices is not None and len(hull_indices) > 3:\n",
        "                    defects = cv2.convexityDefects(contour, hull_indices)\n",
        "                    concave_points_count = 0\n",
        "                    if defects is not None:\n",
        "                        min_defect_depth_ratio = 0.05 # Heuristic: defect depth must be at least 5% of diameter\n",
        "                        min_depth_pixels = min_defect_depth_ratio * equivalent_diameter\n",
        "                        for i in range(defects.shape[0]):\n",
        "                            s, e, f, d = defects[i, 0]\n",
        "                            # s: start_index, e: end_index, f: farthest_point_index, d: depth (fixed-point)\n",
        "                            depth = d / 256.0 # Convert depth to pixels\n",
        "                            if depth > min_depth_pixels: \n",
        "                                concave_points_count += 1\n",
        "                        features['concave_points'] = concave_points_count\n",
        "                    else:\n",
        "                        features['concave_points'] = 0\n",
        "                else:\n",
        "                    features['concave_points'] = 0\n",
        "            else:\n",
        "                features['concave_points'] = 0\n",
        "        except cv2.error: # Catch OpenCV errors during defect calculation\n",
        "            features['concave_points'] = 0\n",
        "\n",
        "        # Symmetry (ratio of minor to major axis of fitted ellipse)\n",
        "        if len(contour) >= 5: # fitEllipse needs at least 5 points\n",
        "            try:\n",
        "                (x, y), (ma, MA), angle = cv2.fitEllipse(contour)\n",
        "                features['symmetry'] = ma / MA if MA > 0 else 1.0 # Ratio of minor to major axis\n",
        "            except cv2.error: # Handle cases where ellipse fitting fails\n",
        "                 features['symmetry'] = 0.0 # Or some other default for non-fittable shapes\n",
        "        else:\n",
        "            features['symmetry'] = 0.0 # Not enough points to fit ellipse\n",
        "\n",
        "        # Fractal dimension (approximate, e.g., perimeter / sqrt(area))\n",
        "        features['fractal_dimension'] = perimeter / np.sqrt(area) if area > 0 else 0.0\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Error calculating features for a contour: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Gather Image Paths and Split Data (Patient-Level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_image_paths_info = []\n",
        "print(f\"Scanning directory: {ROOT_DATA_DIR}...\")\n",
        "\n",
        "for root, dirs, files in os.walk(ROOT_DATA_DIR):\n",
        "    path_parts = list(pathlib.Path(root).parts)\n",
        "    # Example path: ./raw/malignant/SOB/ductal_carcinoma/SOB_M_DC_14-10926/40X\n",
        "    # We need at least up to patient_id and magnification folder\n",
        "    if len(path_parts) >= 7 and 'X' in path_parts[-1] and files:\n",
        "        try:\n",
        "            magnification = path_parts[-1]\n",
        "            patient_slide_id = path_parts[-2] # e.g., SOB_M_DC_14-10926\n",
        "            # sub_diagnosis_type = path_parts[-3] # e.g., ductal_carcinoma\n",
        "            # sob_or_cnb = path_parts[-4] # e.g., SOB\n",
        "            main_diagnosis_folder = path_parts[-5] # e.g., malignant or benign\n",
        "\n",
        "            if main_diagnosis_folder.lower() not in ['benign', 'malignant']:\n",
        "                continue # Skip if not a primary diagnosis folder\n",
        "            \n",
        "            diagnosis_label = 'M' if main_diagnosis_folder.lower() == 'malignant' else 'B'\n",
        "            \n",
        "            # Check if magnification is configured\n",
        "            if magnification not in SEGMENTATION_CONFIG['contour_filters_by_magnification']:\n",
        "                # print(f\"Skipping unconfigured magnification: {magnification} in {root}\")\n",
        "                continue\n",
        "\n",
        "            for file_name in files:\n",
        "                if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n",
        "                    all_image_paths_info.append({\n",
        "                        'path': os.path.join(root, file_name),\n",
        "                        'filename': file_name,\n",
        "                        'diagnosis': diagnosis_label,\n",
        "                        'magnification': magnification,\n",
        "                        'patient_slide_id': patient_slide_id\n",
        "                    })\n",
        "        except IndexError:\n",
        "            # print(f\"Skipping path due to unexpected structure: {root}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing path info for {root}: {e}\")\n",
        "\n",
        "print(f\"Found {len(all_image_paths_info)} total relevant images from {len(set(item['patient_slide_id'] for item in all_image_paths_info))} unique patient/slides.\")\n",
        "\n",
        "train_image_info_list, test_image_info_list = [], []\n",
        "df_all_images = pd.DataFrame()\n",
        "\n",
        "if not all_image_paths_info:\n",
        "    print(\"CRITICAL ERROR: No images found. Pipeline cannot continue.\")\n",
        "else:\n",
        "    df_all_images = pd.DataFrame(all_image_paths_info)\n",
        "    print(f\"\\nSplitting patient/slide IDs into Train/Test sets (Patient-Level Split)...\")\n",
        "    \n",
        "    unique_patient_slide_ids = df_all_images['patient_slide_id'].unique()\n",
        "    # For stratification, get the diagnosis associated with each patient/slide ID (e.g., from its first image entry)\n",
        "    patient_diagnoses = df_all_images.groupby('patient_slide_id')['diagnosis'].first()\n",
        "\n",
        "    if len(unique_patient_slide_ids) < 2 or (len(unique_patient_slide_ids) >=2 and len(patient_diagnoses.loc[unique_patient_slide_ids].unique()) < 2) :\n",
        "        print(f\"Warning: Not enough unique patient/slide IDs ({len(unique_patient_slide_ids)}) or distinct diagnosis groups among them for stratified patient-level split.\")\n",
        "        if len(unique_patient_slide_ids) == 1:\n",
        "            print(\"Only one patient/slide ID found. Assigning all to training set.\")\n",
        "            train_patient_ids = unique_patient_slide_ids\n",
        "            test_patient_ids = np.array([])\n",
        "        elif len(unique_patient_slide_ids) > 1 : # Can do random split\n",
        "            print(\"Attempting random split of patient/slide IDs instead.\")\n",
        "            train_patient_ids, test_patient_ids = train_test_split(\n",
        "                unique_patient_slide_ids, test_size=TEST_SET_SIZE, random_state=RANDOM_STATE\n",
        "            )\n",
        "        else: # No patients\n",
        "            train_patient_ids, test_patient_ids = np.array([]), np.array([])\n",
        "    else:\n",
        "        try:\n",
        "            train_patient_ids, test_patient_ids = train_test_split(\n",
        "                unique_patient_slide_ids, \n",
        "                test_size=TEST_SET_SIZE, \n",
        "                random_state=RANDOM_STATE, \n",
        "                stratify=patient_diagnoses.loc[unique_patient_slide_ids] # Stratify on the series of patient diagnoses\n",
        "            )\n",
        "        except ValueError as e:\n",
        "            print(f\"Error during stratified patient-level split: {e}. Falling back to random split.\")\n",
        "            train_patient_ids, test_patient_ids = train_test_split(\n",
        "                unique_patient_slide_ids, test_size=TEST_SET_SIZE, random_state=RANDOM_STATE\n",
        "            )\n",
        "\n",
        "    if train_patient_ids.size > 0:\n",
        "        train_image_info_list = df_all_images[df_all_images['patient_slide_id'].isin(train_patient_ids)].to_dict('records')\n",
        "    if test_patient_ids.size > 0:\n",
        "        test_image_info_list = df_all_images[df_all_images['patient_slide_id'].isin(test_patient_ids)].to_dict('records')\n",
        "\n",
        "    print(f\"Total unique patient/slide IDs: {len(unique_patient_slide_ids)}\")\n",
        "    print(f\"Training patient/slide IDs: {len(train_patient_ids)}\")\n",
        "    print(f\"Testing patient/slide IDs:  {len(test_patient_ids)}\")\n",
        "    print(f\"Training image count: {len(train_image_info_list)}\")\n",
        "    print(f\"Testing image count:  {len(test_image_info_list)}\")\n",
        "\n",
        "    # Sanity Check for patient overlap\n",
        "    if train_patient_ids.size > 0 and test_patient_ids.size > 0:\n",
        "        common_patients = set(train_patient_ids).intersection(set(test_patient_ids))\n",
        "        if common_patients:\n",
        "            print(f\"CRITICAL ERROR IN SPLIT: Patients found in both train and test sets: {common_patients}\")\n",
        "        else:\n",
        "            print(\"Patient-level split successful: No patient/slide ID overlap between train and test sets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Extraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_features_for_paths(image_info_list, desc=\"Extracting Features\"):\n",
        "    all_features_data = []\n",
        "    # Define output columns including ID, Diagnosis, Magnification, and all feature columns\n",
        "    output_cols = ['ID', 'Diagnosis', 'Magnification', 'PatientSlideID'] + FEATURE_COLUMNS\n",
        "\n",
        "    for img_info in tqdm(image_info_list, desc=desc):\n",
        "        img_path = img_info['path']\n",
        "        image_id = img_info['filename'] # Using filename as ID for the row\n",
        "        diagnosis = img_info['diagnosis']\n",
        "        magnification = img_info['magnification']\n",
        "        patient_slide_id = img_info['patient_slide_id']\n",
        "\n",
        "        # Initialize a dictionary for this image's row with NaNs for all feature columns\n",
        "        row_result = {'ID': image_id, 'Diagnosis': diagnosis, \n",
        "                      'Magnification': magnification, 'PatientSlideID': patient_slide_id}\n",
        "        for col in FEATURE_COLUMNS: \n",
        "            row_result[col] = np.nan \n",
        "\n",
        "        try:\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is None:\n",
        "                print(f\"Warning: Could not read image {img_path}. Skipping.\")\n",
        "                all_features_data.append(row_result) # Append row with NaNs\n",
        "                continue\n",
        "\n",
        "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "            \n",
        "            # Use the new advanced segmentation function\n",
        "            nuclei_contours = segment_nuclei_pipeline(image, magnification, SEGMENTATION_CONFIG)\n",
        "\n",
        "            if not nuclei_contours:\n",
        "                all_features_data.append(row_result) # No nuclei found, append row with NaNs\n",
        "                continue\n",
        "\n",
        "            # Calculate features for each detected nucleus\n",
        "            nucleus_features_list = []\n",
        "            for contour in nuclei_contours:\n",
        "                features = calculate_contour_features(contour, gray_image)\n",
        "                if features: # If feature calculation was successful\n",
        "                    nucleus_features_list.append(features)\n",
        "            \n",
        "            if not nucleus_features_list:\n",
        "                all_features_data.append(row_result) # No valid features calculated, append NaNs\n",
        "                continue\n",
        "\n",
        "            df_nuclei = pd.DataFrame(nucleus_features_list)\n",
        "            num_nuclei = len(df_nuclei)\n",
        "\n",
        "            # Aggregate features (mean, standard error, max of largest/worst)\n",
        "            for base_feature_name in FEATURE_NAMES_BASE:\n",
        "                if base_feature_name in df_nuclei.columns:\n",
        "                    feature_values = pd.to_numeric(df_nuclei[base_feature_name], errors='coerce').dropna()\n",
        "                    if not feature_values.empty:\n",
        "                        row_result[f'{base_feature_name}_mean'] = feature_values.mean()\n",
        "                        # Standard Error of the Mean (SEM)\n",
        "                        row_result[f'{base_feature_name}_se'] = feature_values.std() / math.sqrt(num_nuclei) if num_nuclei > 1 else 0.0\n",
        "                        # For '_max', it's often interpreted as the 'worst' or largest value among the nuclei\n",
        "                        row_result[f'{base_feature_name}_max'] = feature_values.max()\n",
        "            \n",
        "            all_features_data.append(row_result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}. Appending row with NaNs.\")\n",
        "            all_features_data.append(row_result) # Append row with NaNs in case of any error\n",
        "            \n",
        "    return pd.DataFrame(all_features_data, columns=output_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Extract Features for Training and Testing Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train_features = pd.DataFrame(columns=['ID', 'Diagnosis', 'Magnification', 'PatientSlideID'] + FEATURE_COLUMNS)\n",
        "df_test_features = pd.DataFrame(columns=['ID', 'Diagnosis', 'Magnification', 'PatientSlideID'] + FEATURE_COLUMNS)\n",
        "\n",
        "if train_image_info_list:\n",
        "    print(\"\\n--- Extracting Features for Training Set ---\")\n",
        "    df_train_features = extract_features_for_paths(train_image_info_list, desc=\"Training Set Features\")\n",
        "    print(f\"Training features extracted. Shape: {df_train_features.shape}\")\n",
        "    print(df_train_features.head())\n",
        "else:\n",
        "    print(\"\\nSkipping training feature extraction: No training images.\")\n",
        "\n",
        "if test_image_info_list:\n",
        "    print(\"\\n--- Extracting Features for Testing Set ---\")\n",
        "    df_test_features = extract_features_for_paths(test_image_info_list, desc=\"Testing Set Features\")\n",
        "    print(f\"Testing features extracted. Shape: {df_test_features.shape}\")\n",
        "    print(df_test_features.head())\n",
        "else:\n",
        "    print(\"\\nSkipping testing feature extraction: No testing images.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Cleaning & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "le = LabelEncoder()\n",
        "imputer = SimpleImputer(strategy='mean') # Using mean for imputation if chosen\n",
        "\n",
        "X_train_scaled = pd.DataFrame() \n",
        "y_train = np.array([])\n",
        "X_test_scaled = pd.DataFrame()\n",
        "y_test = np.array([])\n",
        "\n",
        "if not df_train_features.empty:\n",
        "    print(\"\\n--- Preprocessing Training Data ---\")\n",
        "    df_train_clean = df_train_features.copy()\n",
        "    initial_train_rows = df_train_clean.shape[0]\n",
        "    \n",
        "    # NaN handling for FEATURE_COLUMNS only\n",
        "    nan_rows_train_count = df_train_clean[FEATURE_COLUMNS].isnull().any(axis=1).sum()\n",
        "    if nan_rows_train_count > 0:\n",
        "        if NAN_HANDLING_STRATEGY == 'drop':\n",
        "            print(f\"Strategy: Dropping {nan_rows_train_count} training rows with NaNs in features.\")\n",
        "            df_train_clean.dropna(subset=FEATURE_COLUMNS, inplace=True)\n",
        "        elif NAN_HANDLING_STRATEGY == 'impute_mean':\n",
        "            print(f\"Strategy: Imputing NaNs in training features with column mean.\")\n",
        "            df_train_clean[FEATURE_COLUMNS] = imputer.fit_transform(df_train_clean[FEATURE_COLUMNS])\n",
        "    print(f\"Training rows after NaN handling: {df_train_clean.shape[0]} (out of {initial_train_rows})\")\n",
        "\n",
        "    if not df_train_clean.empty:\n",
        "        X_train = df_train_clean[FEATURE_COLUMNS]\n",
        "        y_train_raw = df_train_clean['Diagnosis']\n",
        "        y_train = le.fit_transform(y_train_raw)\n",
        "        print(f\"Target variable 'Diagnosis' encoded. Classes: {le.classes_}\")\n",
        "        \n",
        "        X_train_scaled_np = scaler.fit_transform(X_train)\n",
        "        X_train_scaled = pd.DataFrame(X_train_scaled_np, columns=FEATURE_COLUMNS, index=X_train.index)\n",
        "        print(\"Training features scaled.\")\n",
        "    else:\n",
        "        print(\"Training data is empty after NaN handling. Cannot proceed with training.\")\n",
        "else:\n",
        "    print(\"\\nTraining features dataframe is empty. Cannot preprocess.\")\n",
        "\n",
        "\n",
        "if not df_test_features.empty and not X_train_scaled.empty: # Ensure training preprocessing happened\n",
        "    print(\"\\n--- Preprocessing Testing Data ---\")\n",
        "    df_test_clean = df_test_features.copy()\n",
        "    initial_test_rows = df_test_clean.shape[0]\n",
        "\n",
        "    nan_rows_test_count = df_test_clean[FEATURE_COLUMNS].isnull().any(axis=1).sum()\n",
        "    if nan_rows_test_count > 0:\n",
        "        if NAN_HANDLING_STRATEGY == 'drop':\n",
        "            print(f\"Strategy: Dropping {nan_rows_test_count} testing rows with NaNs in features.\")\n",
        "            df_test_clean.dropna(subset=FEATURE_COLUMNS, inplace=True)\n",
        "        elif NAN_HANDLING_STRATEGY == 'impute_mean':\n",
        "            print(f\"Strategy: Imputing NaNs in testing features using imputer fitted on training data.\")\n",
        "            try:\n",
        "                df_test_clean[FEATURE_COLUMNS] = imputer.transform(df_test_clean[FEATURE_COLUMNS])\n",
        "            except NotFittedError:\n",
        "                print(\"Error: Imputer not fitted (likely no NaNs in training or impute strategy not used). Dropping test NaNs.\")\n",
        "                df_test_clean.dropna(subset=FEATURE_COLUMNS, inplace=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error imputing test NaNs: {e}. Dropping test NaNs.\")\n",
        "                df_test_clean.dropna(subset=FEATURE_COLUMNS, inplace=True)\n",
        "    print(f\"Testing rows after NaN handling: {df_test_clean.shape[0]} (out of {initial_test_rows})\")\n",
        "\n",
        "    if not df_test_clean.empty:\n",
        "        X_test = df_test_clean[FEATURE_COLUMNS]\n",
        "        y_test_raw = df_test_clean['Diagnosis']\n",
        "        try:\n",
        "            y_test = le.transform(y_test_raw) # Use already fitted LabelEncoder\n",
        "            print(\"Test target encoded.\")\n",
        "        except ValueError as e: \n",
        "            print(f\"Warning: Error encoding test labels (some labels might be new or missing): {e}.\")\n",
        "            # Handle new labels - for now, we might just assign a placeholder or filter these rows\n",
        "            # A robust way is to ensure all labels in test are also in train, or handle them gracefully.\n",
        "            # For simplicity here, we'll try to proceed but evaluation might be tricky.\n",
        "            y_test = np.array([-1] * len(y_test_raw)) # Placeholder for unencodeable labels\n",
        "        \n",
        "        try:\n",
        "            X_test_scaled_np = scaler.transform(X_test) # Use already fitted Scaler\n",
        "            X_test_scaled = pd.DataFrame(X_test_scaled_np, columns=FEATURE_COLUMNS, index=X_test.index)\n",
        "            print(\"Testing features scaled.\")\n",
        "        except NotFittedError:\n",
        "            print(\"Error: Scaler not fitted (training data might have been empty). Cannot scale test data.\")\n",
        "            X_test_scaled = pd.DataFrame() # Ensure it's empty\n",
        "        except Exception as e:\n",
        "            print(f\"Error scaling test data: {e}\")\n",
        "            X_test_scaled = pd.DataFrame()\n",
        "    else:\n",
        "        print(\"Testing data is empty after NaN handling.\")\n",
        "elif df_test_features.empty:\n",
        "    print(\"\\nTesting features dataframe is empty. Cannot preprocess.\")\n",
        "else: # X_train_scaled is empty\n",
        "    print(\"\\nTraining data was not processed. Skipping test data preprocessing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Training (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm_model = None # Initialize model variable\n",
        "\n",
        "if not X_train_scaled.empty and y_train.size > 0:\n",
        "    print(\"\\n--- Training SVM Model ---\")\n",
        "    # Common Kernels: 'rbf' (default), 'linear', 'poly'\n",
        "    # probability=True is needed for ROC curve but slows down training.\n",
        "    # class_weight='balanced' helps with imbalanced datasets.\n",
        "    svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=RANDOM_STATE, class_weight='balanced')\n",
        "\n",
        "    print(f\"Training SVC with parameters: {svm_model.get_params()}\")\n",
        "    try:\n",
        "        svm_model.fit(X_train_scaled, y_train)\n",
        "        print(\"SVM Model Trained.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SVM model training: {e}\")\n",
        "        svm_model = None # Ensure model is None if training failed\n",
        "else:\n",
        "    print(\"\\nSkipping model training due to lack of processed training data or labels.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model_name, model, X_test_data, y_true_encoded, le_encoder):\n",
        "    print(f\"\\n--- Evaluation Metrics for: {model_name} on Test Set ---\")\n",
        "    if model is None: \n",
        "        print(\"Model was not trained or training failed.\"); return 0.0, None, None\n",
        "    if X_test_data.empty: \n",
        "        print(\"Test data is empty.\"); return 0.0, None, None\n",
        "    if len(y_true_encoded) == 0 or np.all(y_true_encoded == -1): \n",
        "        print(\"Test labels are invalid or empty.\"); return 0.0, None, None\n",
        "    \n",
        "    y_pred, y_prob = None, None\n",
        "    try:\n",
        "        y_pred = model.predict(X_test_data)\n",
        "        if hasattr(model, \"predict_proba\"): # Check if model supports predict_proba\n",
        "             y_prob = model.predict_proba(X_test_data)[:, 1] # Probability of the positive class\n",
        "        else:\n",
        "            print(\"Model does not support predict_proba, ROC AUC cannot be calculated.\")\n",
        "    except NotFittedError:\n",
        "        print(\"Error: Model is not fitted. Cannot make predictions.\")\n",
        "        return 0.0, None, None\n",
        "    except Exception as e: \n",
        "        print(f\"Error during prediction: {e}\"); return 0.0, None, None\n",
        "\n",
        "    accuracy = accuracy_score(y_true_encoded, y_pred)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    try:\n",
        "        # Ensure target_names are generated based on the classes present in y_true_encoded and y_pred\n",
        "        # This handles cases where test set might not have all classes seen by le\n",
        "        present_labels = np.unique(np.concatenate((y_true_encoded, y_pred)))\n",
        "        present_target_names = le_encoder.inverse_transform(present_labels)\n",
        "        print(classification_report(y_true_encoded, y_pred, labels=present_labels, target_names=present_target_names, zero_division=0))\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate full classification report: {e}\")\n",
        "        print(classification_report(y_true_encoded, y_pred, zero_division=0))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(y_true_encoded, y_pred, labels=le_encoder.transform(le_encoder.classes_))\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=le_encoder.classes_, yticklabels=le_encoder.classes_)\n",
        "    plt.xlabel('Predicted Label'); plt.ylabel('True Label'); plt.title(f'Confusion Matrix - {model_name}'); plt.show()\n",
        "\n",
        "    roc_auc = None\n",
        "    if y_prob is not None and len(np.unique(y_true_encoded)) > 1: # ROC AUC needs at least 2 classes in y_true\n",
        "        fpr, tpr, thresholds = roc_curve(y_true_encoded, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        print(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n",
        "        plt.figure(figsize=(7, 5))\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--'); plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title(f'ROC - {model_name}'); plt.legend(loc=\"lower right\"); plt.show()\n",
        "    elif y_prob is None:\n",
        "        print(\"ROC AUC cannot be calculated as y_prob is not available.\")\n",
        "    else:\n",
        "        print(\"ROC AUC cannot be calculated because there is only one class present in y_true.\")\n",
        "        \n",
        "    return accuracy, cm, roc_auc\n",
        "\n",
        "# --- Evaluate SVM Model ---\n",
        "acc_svm, cm_svm, roc_auc_svm = 0.0, None, None\n",
        "if svm_model is not None and not X_test_scaled.empty and y_test.size > 0:\n",
        "    print(\"\\n\\n=== Evaluating SVM Model on Test Set ===\")\n",
        "    acc_svm, cm_svm, roc_auc_svm = evaluate_model(\"SVM (RBF Kernel)\", svm_model, X_test_scaled, y_test, le)\n",
        "elif X_test_scaled.empty or y_test.size == 0:\n",
        "    print(\"\\nSkipping model evaluation: Test data or labels are empty or invalid.\")\n",
        "else:\n",
        "    print(\"\\nSkipping model evaluation: SVM model was not trained.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Conclusion & Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\\n--- Final Summary ---\")\n",
        "if not df_all_images.empty:\n",
        "    print(f\"Processed {len(all_image_paths_info)} total images from {df_all_images['patient_slide_id'].nunique()} unique patient/slides.\")\n",
        "    print(f\"Used {len(train_image_info_list)} images for training feature extraction (from {len(set(item['patient_slide_id'] for item in train_image_info_list))} patients/slides).\")\n",
        "    print(f\"Used {len(test_image_info_list)} images for testing feature extraction (from {len(set(item['patient_slide_id'] for item in test_image_info_list))} patients/slides).\")\n",
        "\n",
        "if not X_train_scaled.empty:\n",
        "    print(f\"Training data samples after cleaning: {X_train_scaled.shape[0]}\")\n",
        "else:\n",
        "    print(\"Training data: N/A or empty after cleaning.\")\n",
        "\n",
        "if not X_test_scaled.empty:\n",
        "    print(f\"Testing data samples after cleaning: {X_test_scaled.shape[0]}\")\n",
        "else:\n",
        "    print(\"Testing data: N/A or empty after cleaning.\")\n",
        "\n",
        "if svm_model is not None and acc_svm > 0.0: # Check if evaluation was successful\n",
        "    print(f\"\\nTest Set Accuracy (SVM): {acc_svm:.4f}\")\n",
        "    if roc_auc_svm is not None:\n",
        "        print(f\"Test Set ROC AUC (SVM): {roc_auc_svm:.4f}\")\n",
        "    print(\"\\nReview detailed metrics (precision, recall, F1, Confusion Matrix) above.\")\n",
        "elif svm_model is not None:\n",
        "    print(\"\\nSVM Model was trained, but evaluation on test set could not be completed (e.g., empty test set after cleaning).\")\n",
        "else:\n",
        "    print(\"\\nFull pipeline could not be completed or model training/evaluation failed.\")\n",
        "\n",
        "print(\"\\nConsiderations & Next Steps:\")\n",
        "print(\"- Segmentation Parameters: Ensure `SEGMENTATION_CONFIG` reflects your best tuned values.\")\n",
        "print(\"- SVM Hyperparameters: Use GridSearchCV or RandomizedSearchCV on the *training set* to tune `C`, `gamma`, and `kernel`.\")\n",
        "print(\"- Patient-Level Split: Verify it's working correctly and robustly.\")\n",
        "print(\"- NaN Handling: Assess impact of 'drop' vs. 'impute_mean'. 'impute_mean' might be better if dropping removes too much data.\")\n",
        "print(\"- Model Comparison: Train other models (e.g., Random Forest, Gradient Boosting) using the same train/test split for comparison.\")\n",
        "print(\"- Feature Engineering/Selection: Explore if all features are useful or if new ones could be added.\")\n",
        "print(\"- Data Augmentation (if using direct image input for Deep Learning later): Consider if images are diverse enough.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
