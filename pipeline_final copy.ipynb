{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Breast Cancer Histology: Full Pipeline with Advanced Segmentation\n",
        "\n",
        "This notebook implements the complete workflow using an SVM classifier and an advanced nuclei segmentation strategy:\n",
        "1. Finds all relevant image paths and extracts patient/slide IDs.\n",
        "2. Splits patient/slide IDs into Training and Testing sets (patient-level split).\n",
        "3. Extracts features ONLY from the Training image set using advanced segmentation.\n",
        "4. Cleans, preprocesses (scaling), and trains an SVM model on the Training features.\n",
        "5. Extracts features ONLY from the Testing image set using advanced segmentation.\n",
        "6. Cleans and preprocesses (using the scaler fitted on Training data) the Testing features.\n",
        "7. Evaluates the trained SVM model on the unseen Testing features.\n",
        "\n",
        "**Advanced Segmentation Strategy:**\n",
        "*   Color Deconvolution (Hematoxylin channel isolation)\n",
        "*   Adaptive Thresholding (Otsu) or Manual Thresholding\n",
        "*   Morphological Operations (Opening, Closing)\n",
        "*   Watershed Segmentation (Optional, for separating nuclei)\n",
        "*   Contour Filtering (Area, Circularity, per magnification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "import pathlib\n",
        "import json # For potentially loading/displaying config\n",
        "\n",
        "from tqdm.notebook import tqdm # Use tqdm.notebook for Jupyter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# ... (existing imports) ...\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb # Import XGBoost\n",
        "\n",
        "\n",
        "# Scikit-image for advanced segmentation\n",
        "from skimage import color, exposure\n",
        "# from scipy import ndimage # Not strictly needed if cv2.connectedComponents is used\n",
        "# ... (other imports)\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold # Added GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    precision_recall_fscore_support\n",
        ")\n",
        "from sklearn.exceptions import NotFittedError\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Path to Dataset ---\n",
        "notebook_launch_dir = pathlib.Path.cwd()\n",
        "ROOT_DATA_DIR = notebook_launch_dir / 'raw'\n",
        "\n",
        "# --- Modeling Parameters ---\n",
        "TEST_SET_SIZE = 0.25  # Proportion of unique patient/slide IDs for the test set\n",
        "RANDOM_STATE = 42\n",
        "NAN_HANDLING_STRATEGY = 'impute_mean' # 'drop' or 'impute_mean'\n",
        "\n",
        "# --- Feature Names ---\n",
        "FEATURE_NAMES_BASE = ['radius', 'texture', 'perimeter', 'area', 'smoothness', \n",
        "                      'compactness', 'concavity', 'concave_points', 'symmetry', \n",
        "                      'fractal_dimension']\n",
        "# For Mean, Standard Error (SE), and Max of worst/largest nuclei features\n",
        "FEATURE_COLUMNS = [f'{name}{suffix}' for name in FEATURE_NAMES_BASE for suffix in ['_mean', '_se', '_max']]\n",
        "\n",
        "# --- YOUR TUNED SEGMENTATION CONFIGURATION ---\n",
        "# IMPORTANT: Replace this example with the actual SEGMENTATION_CONFIG \n",
        "# dictionary that you fine-tuned using the Gradio UI or manual iteration.\n",
        "\n",
        "SEGMENTATION_CONFIG = {\n",
        "  \"contrast_stretch\": False,\n",
        "  \"contrast_percentiles_low\": 2,\n",
        "  \"contrast_percentiles_high\": 98,\n",
        "  \"threshold_method\": \"otsu\",\n",
        "  \"manual_threshold_value\": 100,\n",
        "  \"morph_open_kernel_size\": 3,\n",
        "  \"morph_open_iterations\": 1,\n",
        "  \"morph_close_kernel_size\": 3,\n",
        "  \"morph_close_iterations\": 1,\n",
        "  \"use_watershed\": False,\n",
        "  \"dist_transform_thresh_ratio\": 0.3,\n",
        "  \"contour_filters_by_magnification\": {\n",
        "    \"40X\": {\n",
        "      \"min_area\": 10,\n",
        "      \"max_area\": 300,\n",
        "      \"min_circularity\": 0.3,\n",
        "      \"dist_transform_thresh_ratio\": 0.2\n",
        "    },\n",
        "    \"100X\": {\n",
        "      \"min_area\": 40,\n",
        "      \"max_area\": 1000,\n",
        "      \"min_circularity\": 0.3,\n",
        "      \"dist_transform_thresh_ratio\": 0.3\n",
        "    },\n",
        "    \"200X\": {\n",
        "      \"min_area\": 10,\n",
        "      \"max_area\": 3000,\n",
        "      \"min_circularity\": 0.5,\n",
        "      \"dist_transform_thresh_ratio\": 0.5\n",
        "    },\n",
        "    \"400X\": {\n",
        "      \"min_area\": 20,\n",
        "      \"max_area\": 7000,\n",
        "      \"min_circularity\": 0.2,\n",
        "      \"dist_transform_thresh_ratio\": 0.5\n",
        "    }\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Helper Functions (Segmentation, Feature Calculation, Evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_hematoxylin_channel(image_rgb):\n",
        "    \"\"\"Extracts the Hematoxylin channel using scikit-image's color deconvolution.\"\"\"\n",
        "    # Add a small epsilon to prevent division by zero or log of zero if image has pure black pixels\n",
        "    image_rgb_safe = np.clip(image_rgb, 1, 255) \n",
        "    ihc_hed = color.rgb2hed(image_rgb_safe)\n",
        "    h_channel = ihc_hed[:, :, 0]\n",
        "    # Normalize to 0-255, nuclei should be bright (higher values)\n",
        "    h_channel_norm = cv2.normalize(h_channel, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "    return h_channel_norm\n",
        "\n",
        "def calculate_circularity(contour):\n",
        "    \"\"\"Calculates the circularity of a contour.\"\"\"\n",
        "    perimeter = cv2.arcLength(contour, True)\n",
        "    area = cv2.contourArea(contour)\n",
        "    if perimeter == 0 or area == 0:\n",
        "        return 0\n",
        "    return (4 * np.pi * area) / (perimeter ** 2)\n",
        "\n",
        "def segment_nuclei_pipeline(image_bgr, magnification, config):\n",
        "    \"\"\"Advanced nuclei segmentation for pipeline use. Returns only filtered contours.\"\"\"\n",
        "    if image_bgr is None: return []\n",
        "\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    h_channel = get_hematoxylin_channel(image_rgb)\n",
        "    h_channel_processed = h_channel.copy()\n",
        "\n",
        "    if config.get('contrast_stretch', False):\n",
        "        p_low = config['contrast_percentiles_low']\n",
        "        p_high = config['contrast_percentiles_high']\n",
        "        if 0 <= p_low < p_high <= 100:\n",
        "            p_low_val, p_high_val = np.percentile(h_channel_processed, (p_low, p_high))\n",
        "            if p_low_val < p_high_val: # Check if percentiles are distinct\n",
        "                h_channel_processed = exposure.rescale_intensity(h_channel_processed, in_range=(p_low_val, p_high_val))\n",
        "                h_channel_processed = cv2.normalize(h_channel_processed, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "\n",
        "    if config['threshold_method'] == 'otsu':\n",
        "        _, binary_mask = cv2.threshold(h_channel_processed, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    else: # manual\n",
        "        _, binary_mask = cv2.threshold(h_channel_processed, config['manual_threshold_value'], 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    open_k_size = config['morph_open_kernel_size']\n",
        "    if open_k_size % 2 == 0: open_k_size +=1 # Ensure odd kernel size\n",
        "    close_k_size = config['morph_close_kernel_size']\n",
        "    if close_k_size % 2 == 0: close_k_size +=1 # Ensure odd kernel size\n",
        "    \n",
        "    kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (open_k_size, open_k_size))\n",
        "    opened_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel_open, iterations=config['morph_open_iterations'])\n",
        "    \n",
        "    kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (close_k_size, close_k_size))\n",
        "    cleaned_mask = cv2.morphologyEx(opened_mask, cv2.MORPH_CLOSE, kernel_close, iterations=config['morph_close_iterations'])\n",
        "\n",
        "    final_contours_list = []\n",
        "    # Get magnification-specific filters, fallback to '200X' if current mag not found (should not happen with good config)\n",
        "    mag_specific_filters = config['contour_filters_by_magnification'].get(magnification, \n",
        "                                                                        config['contour_filters_by_magnification'].get('200X'))\n",
        "\n",
        "    if config.get('use_watershed', True):\n",
        "        sure_bg_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3)) \n",
        "        sure_bg = cv2.dilate(cleaned_mask, sure_bg_kernel, iterations=3)\n",
        "        dist_transform = cv2.distanceTransform(cleaned_mask, cv2.DIST_L2, 5)\n",
        "        \n",
        "        default_global_dt_ratio = config.get('dist_transform_thresh_ratio', 0.3) # Global fallback from main config\n",
        "        current_dist_thresh_ratio = mag_specific_filters.get('dist_transform_thresh_ratio', default_global_dt_ratio)\n",
        "        \n",
        "        _, sure_fg = cv2.threshold(dist_transform, current_dist_thresh_ratio * dist_transform.max(), 255, 0)\n",
        "        sure_fg = np.uint8(sure_fg)\n",
        "        unknown = cv2.subtract(sure_bg, sure_fg)\n",
        "        _, markers = cv2.connectedComponents(sure_fg)\n",
        "        markers = markers + 1 # So background is 1, not 0\n",
        "        markers[unknown == 255] = 0 # Mark unknown region\n",
        "        \n",
        "        try:\n",
        "            markers_copy = markers.copy() # watershed modifies markers in-place\n",
        "            watershed_input_img = image_rgb.copy() # Use a copy for watershed if original image_rgb is needed later\n",
        "            cv2.watershed(watershed_input_img, markers_copy)\n",
        "            \n",
        "            unique_marker_values = np.unique(markers_copy) # Use the modified markers_copy\n",
        "            for marker_val in unique_marker_values:\n",
        "                if marker_val <= 1:  # Skip background (1) and watershed boundary (-1)\n",
        "                    continue\n",
        "                nucleus_mask_ws = np.zeros(cleaned_mask.shape, dtype=np.uint8)\n",
        "                nucleus_mask_ws[markers_copy == marker_val] = 255\n",
        "                contours_ws, _ = cv2.findContours(nucleus_mask_ws, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                if contours_ws: final_contours_list.extend(contours_ws)\n",
        "        except cv2.error as e:\n",
        "            # print(f\"Pipeline Watershed failed (mag: {magnification}, file: {os.path.basename(image_bgr.name if hasattr(image_bgr, 'name') else 'N/A')}): {e}. Falling back.\")\n",
        "            contours_fb, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            final_contours_list = contours_fb\n",
        "    else: # No watershed\n",
        "        contours_no_ws, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        final_contours_list = contours_no_ws\n",
        "\n",
        "    min_area = mag_specific_filters['min_area']\n",
        "    max_area = mag_specific_filters['max_area']\n",
        "    min_circ = mag_specific_filters['min_circularity']\n",
        "    \n",
        "    filtered_contours = []\n",
        "    for cnt in final_contours_list:\n",
        "        area = cv2.contourArea(cnt)\n",
        "        if min_area < area < max_area:\n",
        "            circularity = calculate_circularity(cnt)\n",
        "            if circularity >= min_circ:\n",
        "                filtered_contours.append(cnt)\n",
        "    return filtered_contours\n",
        "\n",
        "\n",
        "def calculate_contour_features(contour, gray_image):\n",
        "    \"\"\"Calculates a dictionary of features for a single contour.\"\"\"\n",
        "    features = {}\n",
        "    try:\n",
        "        moments = cv2.moments(contour)\n",
        "        area = cv2.contourArea(contour)\n",
        "        if area <= 0: return None # Skip invalid contours\n",
        "\n",
        "        perimeter = cv2.arcLength(contour, True)\n",
        "        equivalent_diameter = np.sqrt(4 * area / np.pi)\n",
        "        features['radius'] = equivalent_diameter / 2.0\n",
        "\n",
        "        # Texture (standard deviation of gray levels within the contour)\n",
        "        mask = np.zeros(gray_image.shape, dtype=np.uint8)\n",
        "        cv2.drawContours(mask, [contour], -1, color=255, thickness=cv2.FILLED)\n",
        "        mean_val, stddev_val = cv2.meanStdDev(gray_image, mask=mask)\n",
        "        features['texture'] = stddev_val[0][0] if stddev_val is not None and stddev_val.size > 0 else 0.0\n",
        "\n",
        "        features['perimeter'] = perimeter\n",
        "        features['area'] = area\n",
        "\n",
        "        # Smoothness (std of distances from center to contour points)\n",
        "        if moments['m00'] > 0 and len(contour) > 1:\n",
        "            cx = int(moments['m10'] / moments['m00'])\n",
        "            cy = int(moments['m01'] / moments['m00'])\n",
        "            center = np.array([cx, cy])\n",
        "            contour_points = contour.reshape(-1, 2)\n",
        "            distances = np.sqrt(np.sum((contour_points - center)**2, axis=1))\n",
        "            features['smoothness'] = np.std(distances) if distances.size > 0 else 0.0\n",
        "        else:\n",
        "            features['smoothness'] = 0.0\n",
        "\n",
        "        # Compactness\n",
        "        features['compactness'] = (perimeter ** 2) / area if area > 0 else 0.0\n",
        "\n",
        "        # Concavity (related to solidity)\n",
        "        hull = cv2.convexHull(contour)\n",
        "        hull_area = cv2.contourArea(hull)\n",
        "        if hull_area > 0:\n",
        "            features['concavity'] = 1.0 - (area / hull_area) # (hull_area - area) / hull_area\n",
        "        else:\n",
        "            features['concavity'] = 0.0\n",
        "\n",
        "        # Concave points (number of significant concavities)\n",
        "        try:\n",
        "            if len(contour) > 3: # Need at least 4 points for convexityDefects\n",
        "                hull_indices = cv2.convexHull(contour, returnPoints=False) # Get indices for defects\n",
        "                if hull_indices is not None and len(hull_indices) > 3:\n",
        "                    defects = cv2.convexityDefects(contour, hull_indices)\n",
        "                    concave_points_count = 0\n",
        "                    if defects is not None:\n",
        "                        min_defect_depth_ratio = 0.05 # Heuristic: defect depth must be at least 5% of diameter\n",
        "                        min_depth_pixels = min_defect_depth_ratio * equivalent_diameter\n",
        "                        for i in range(defects.shape[0]):\n",
        "                            s, e, f, d = defects[i, 0]\n",
        "                            # s: start_index, e: end_index, f: farthest_point_index, d: depth (fixed-point)\n",
        "                            depth = d / 256.0 # Convert depth to pixels\n",
        "                            if depth > min_depth_pixels: \n",
        "                                concave_points_count += 1\n",
        "                        features['concave_points'] = concave_points_count\n",
        "                    else:\n",
        "                        features['concave_points'] = 0\n",
        "                else:\n",
        "                    features['concave_points'] = 0\n",
        "            else:\n",
        "                features['concave_points'] = 0\n",
        "        except cv2.error: # Catch OpenCV errors during defect calculation\n",
        "            features['concave_points'] = 0\n",
        "\n",
        "        # Symmetry (ratio of minor to major axis of fitted ellipse)\n",
        "        if len(contour) >= 5: # fitEllipse needs at least 5 points\n",
        "            try:\n",
        "                (x, y), (ma, MA), angle = cv2.fitEllipse(contour)\n",
        "                features['symmetry'] = ma / MA if MA > 0 else 1.0 # Ratio of minor to major axis\n",
        "            except cv2.error: # Handle cases where ellipse fitting fails\n",
        "                 features['symmetry'] = 0.0 # Or some other default for non-fittable shapes\n",
        "        else:\n",
        "            features['symmetry'] = 0.0 # Not enough points to fit ellipse\n",
        "\n",
        "        # Fractal dimension (approximate, e.g., perimeter / sqrt(area))\n",
        "        features['fractal_dimension'] = perimeter / np.sqrt(area) if area > 0 else 0.0\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Error calculating features for a contour: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Gather Image Paths and Split Data (Patient-Level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_image_paths_info = []\n",
        "print(f\"Scanning directory: {ROOT_DATA_DIR}...\")\n",
        "\n",
        "for root, dirs, files in os.walk(ROOT_DATA_DIR):\n",
        "    path_parts = list(pathlib.Path(root).parts)\n",
        "    # Example path: ./raw/malignant/SOB/ductal_carcinoma/SOB_M_DC_14-10926/40X\n",
        "    # We need at least up to patient_id and magnification folder\n",
        "    if len(path_parts) >= 7 and 'X' in path_parts[-1] and files:\n",
        "        try:\n",
        "            magnification = path_parts[-1]\n",
        "            if magnification != '200X':\n",
        "                continue \n",
        "            patient_slide_id = path_parts[-2] # e.g., SOB_M_DC_14-10926\n",
        "            # sub_diagnosis_type = path_parts[-3] # e.g., ductal_carcinoma\n",
        "            # sob_or_cnb = path_parts[-4] # e.g., SOB\n",
        "            main_diagnosis_folder = path_parts[-5] # e.g., malignant or benign\n",
        "\n",
        "            if main_diagnosis_folder.lower() not in ['benign', 'malignant']:\n",
        "                continue # Skip if not a primary diagnosis folder\n",
        "            \n",
        "            diagnosis_label = 'M' if main_diagnosis_folder.lower() == 'malignant' else 'B'\n",
        "            \n",
        "            # Check if magnification is configured\n",
        "            if magnification not in SEGMENTATION_CONFIG['contour_filters_by_magnification']:\n",
        "                # print(f\"Skipping unconfigured magnification: {magnification} in {root}\")\n",
        "                continue\n",
        "\n",
        "            for file_name in files:\n",
        "                if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n",
        "                    all_image_paths_info.append({\n",
        "                        'path': os.path.join(root, file_name),\n",
        "                        'filename': file_name,\n",
        "                        'diagnosis': diagnosis_label,\n",
        "                        'magnification': magnification,\n",
        "                        'patient_slide_id': patient_slide_id\n",
        "                    })\n",
        "        except IndexError:\n",
        "            # print(f\"Skipping path due to unexpected structure: {root}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing path info for {root}: {e}\")\n",
        "\n",
        "print(f\"Found {len(all_image_paths_info)} total relevant images from {len(set(item['patient_slide_id'] for item in all_image_paths_info))} unique patient/slides.\")\n",
        "\n",
        "train_image_info_list, test_image_info_list = [], []\n",
        "df_all_images = pd.DataFrame()\n",
        "\n",
        "if not all_image_paths_info:\n",
        "    print(\"CRITICAL ERROR: No images found. Pipeline cannot continue.\")\n",
        "else:\n",
        "    df_all_images = pd.DataFrame(all_image_paths_info)\n",
        "    print(f\"\\nSplitting patient/slide IDs into Train/Test sets (Patient-Level Split)...\")\n",
        "    \n",
        "    unique_patient_slide_ids = df_all_images['patient_slide_id'].unique()\n",
        "    # For stratification, get the diagnosis associated with each patient/slide ID (e.g., from its first image entry)\n",
        "    patient_diagnoses = df_all_images.groupby('patient_slide_id')['diagnosis'].first()\n",
        "\n",
        "    if len(unique_patient_slide_ids) < 2 or (len(unique_patient_slide_ids) >=2 and len(patient_diagnoses.loc[unique_patient_slide_ids].unique()) < 2) :\n",
        "        print(f\"Warning: Not enough unique patient/slide IDs ({len(unique_patient_slide_ids)}) or distinct diagnosis groups among them for stratified patient-level split.\")\n",
        "        if len(unique_patient_slide_ids) == 1:\n",
        "            print(\"Only one patient/slide ID found. Assigning all to training set.\")\n",
        "            train_patient_ids = unique_patient_slide_ids\n",
        "            test_patient_ids = np.array([])\n",
        "        elif len(unique_patient_slide_ids) > 1 : # Can do random split\n",
        "            print(\"Attempting random split of patient/slide IDs instead.\")\n",
        "            train_patient_ids, test_patient_ids = train_test_split(\n",
        "                unique_patient_slide_ids, test_size=TEST_SET_SIZE, random_state=RANDOM_STATE\n",
        "            )\n",
        "        else: # No patients\n",
        "            train_patient_ids, test_patient_ids = np.array([]), np.array([])\n",
        "    else:\n",
        "        try:\n",
        "            train_patient_ids, test_patient_ids = train_test_split(\n",
        "                unique_patient_slide_ids, \n",
        "                test_size=TEST_SET_SIZE, \n",
        "                random_state=RANDOM_STATE, \n",
        "                stratify=patient_diagnoses.loc[unique_patient_slide_ids] # Stratify on the series of patient diagnoses\n",
        "            )\n",
        "        except ValueError as e:\n",
        "            print(f\"Error during stratified patient-level split: {e}. Falling back to random split.\")\n",
        "            train_patient_ids, test_patient_ids = train_test_split(\n",
        "                unique_patient_slide_ids, test_size=TEST_SET_SIZE, random_state=RANDOM_STATE\n",
        "            )\n",
        "\n",
        "    if train_patient_ids.size > 0:\n",
        "        train_image_info_list = df_all_images[df_all_images['patient_slide_id'].isin(train_patient_ids)].to_dict('records')\n",
        "    if test_patient_ids.size > 0:\n",
        "        test_image_info_list = df_all_images[df_all_images['patient_slide_id'].isin(test_patient_ids)].to_dict('records')\n",
        "\n",
        "    print(f\"Total unique patient/slide IDs: {len(unique_patient_slide_ids)}\")\n",
        "    print(f\"Training patient/slide IDs: {len(train_patient_ids)}\")\n",
        "    print(f\"Testing patient/slide IDs:  {len(test_patient_ids)}\")\n",
        "    print(f\"Training image count: {len(train_image_info_list)}\")\n",
        "    print(f\"Testing image count:  {len(test_image_info_list)}\")\n",
        "\n",
        "    # Sanity Check for patient overlap\n",
        "    if train_patient_ids.size > 0 and test_patient_ids.size > 0:\n",
        "        common_patients = set(train_patient_ids).intersection(set(test_patient_ids))\n",
        "        if common_patients:\n",
        "            print(f\"CRITICAL ERROR IN SPLIT: Patients found in both train and test sets: {common_patients}\")\n",
        "        else:\n",
        "            print(\"Patient-level split successful: No patient/slide ID overlap between train and test sets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Extraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_features_for_paths(image_info_list, desc=\"Extracting Features\"):\n",
        "    all_features_data = []\n",
        "    # Define output columns including ID, Diagnosis, Magnification, and all feature columns\n",
        "    output_cols = ['ID', 'Diagnosis', 'Magnification', 'PatientSlideID'] + FEATURE_COLUMNS\n",
        "\n",
        "    for img_info in tqdm(image_info_list, desc=desc):\n",
        "        img_path = img_info['path']\n",
        "        image_id = img_info['filename'] # Using filename as ID for the row\n",
        "        diagnosis = img_info['diagnosis']\n",
        "        magnification = img_info['magnification']\n",
        "        patient_slide_id = img_info['patient_slide_id']\n",
        "\n",
        "        # Initialize a dictionary for this image's row with NaNs for all feature columns\n",
        "        row_result = {'ID': image_id, 'Diagnosis': diagnosis, \n",
        "                      'Magnification': magnification, 'PatientSlideID': patient_slide_id}\n",
        "        for col in FEATURE_COLUMNS: \n",
        "            row_result[col] = np.nan \n",
        "\n",
        "        try:\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is None:\n",
        "                print(f\"Warning: Could not read image {img_path}. Skipping.\")\n",
        "                all_features_data.append(row_result) # Append row with NaNs\n",
        "                continue\n",
        "\n",
        "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "            \n",
        "            # Use the new advanced segmentation function\n",
        "            nuclei_contours = segment_nuclei_pipeline(image, magnification, SEGMENTATION_CONFIG)\n",
        "\n",
        "            if not nuclei_contours:\n",
        "                all_features_data.append(row_result) # No nuclei found, append row with NaNs\n",
        "                continue\n",
        "\n",
        "            # Calculate features for each detected nucleus\n",
        "            nucleus_features_list = []\n",
        "            for contour in nuclei_contours:\n",
        "                features = calculate_contour_features(contour, gray_image)\n",
        "                if features: # If feature calculation was successful\n",
        "                    nucleus_features_list.append(features)\n",
        "            \n",
        "            if not nucleus_features_list:\n",
        "                all_features_data.append(row_result) # No valid features calculated, append NaNs\n",
        "                continue\n",
        "\n",
        "            df_nuclei = pd.DataFrame(nucleus_features_list)\n",
        "            num_nuclei = len(df_nuclei)\n",
        "\n",
        "            # Aggregate features (mean, standard error, max of largest/worst)\n",
        "            for base_feature_name in FEATURE_NAMES_BASE:\n",
        "                if base_feature_name in df_nuclei.columns:\n",
        "                    feature_values = pd.to_numeric(df_nuclei[base_feature_name], errors='coerce').dropna()\n",
        "                    if not feature_values.empty:\n",
        "                        row_result[f'{base_feature_name}_mean'] = feature_values.mean()\n",
        "                        # Standard Error of the Mean (SEM)\n",
        "                        row_result[f'{base_feature_name}_se'] = feature_values.std() / math.sqrt(num_nuclei) if num_nuclei > 1 else 0.0\n",
        "                        # For '_max', it's often interpreted as the 'worst' or largest value among the nuclei\n",
        "                        row_result[f'{base_feature_name}_max'] = feature_values.max()\n",
        "            \n",
        "            all_features_data.append(row_result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}. Appending row with NaNs.\")\n",
        "            all_features_data.append(row_result) # Append row with NaNs in case of any error\n",
        "            \n",
        "    return pd.DataFrame(all_features_data, columns=output_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Extract Features for Training and Testing Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train_features = pd.DataFrame(columns=['ID', 'Diagnosis', 'Magnification', 'PatientSlideID'] + FEATURE_COLUMNS)\n",
        "df_test_features = pd.DataFrame(columns=['ID', 'Diagnosis', 'Magnification', 'PatientSlideID'] + FEATURE_COLUMNS)\n",
        "\n",
        "if train_image_info_list:\n",
        "    print(\"\\n--- Extracting Features for Training Set ---\")\n",
        "    df_train_features = extract_features_for_paths(train_image_info_list, desc=\"Training Set Features\")\n",
        "    print(f\"Training features extracted. Shape: {df_train_features.shape}\")\n",
        "    print(df_train_features.head())\n",
        "else:\n",
        "    print(\"\\nSkipping training feature extraction: No training images.\")\n",
        "\n",
        "if test_image_info_list:\n",
        "    print(\"\\n--- Extracting Features for Testing Set ---\")\n",
        "    df_test_features = extract_features_for_paths(test_image_info_list, desc=\"Testing Set Features\")\n",
        "    print(f\"Testing features extracted. Shape: {df_test_features.shape}\")\n",
        "    print(df_test_features.head())\n",
        "else:\n",
        "    print(\"\\nSkipping testing feature extraction: No testing images.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Cleaning & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "le = LabelEncoder()\n",
        "imputer = SimpleImputer(strategy='mean') # Using mean for imputation if chosen\n",
        "\n",
        "X_train_scaled = pd.DataFrame() \n",
        "y_train = np.array([])\n",
        "X_test_scaled = pd.DataFrame()\n",
        "y_test = np.array([])\n",
        "\n",
        "if not df_train_features.empty:\n",
        "    print(\"\\n--- Preprocessing Training Data ---\")\n",
        "    df_train_clean = df_train_features.copy()\n",
        "    initial_train_rows = df_train_clean.shape[0]\n",
        "    \n",
        "    # NaN handling for FEATURE_COLUMNS only\n",
        "    nan_rows_train_count = df_train_clean[FEATURE_COLUMNS].isnull().any(axis=1).sum()\n",
        "    if nan_rows_train_count > 0:\n",
        "        if NAN_HANDLING_STRATEGY == 'drop':\n",
        "            print(f\"Strategy: Dropping {nan_rows_train_count} training rows with NaNs in features.\")\n",
        "            df_train_clean.dropna(subset=FEATURE_COLUMNS, inplace=True)\n",
        "        elif NAN_HANDLING_STRATEGY == 'impute_mean':\n",
        "            print(f\"Strategy: Imputing NaNs in training features with column mean.\")\n",
        "            df_train_clean[FEATURE_COLUMNS] = imputer.fit_transform(df_train_clean[FEATURE_COLUMNS])\n",
        "    print(f\"Training rows after NaN handling: {df_train_clean.shape[0]} (out of {initial_train_rows})\")\n",
        "\n",
        "    if not df_train_clean.empty:\n",
        "        X_train = df_train_clean[FEATURE_COLUMNS]\n",
        "        y_train_raw = df_train_clean['Diagnosis']\n",
        "        y_train = le.fit_transform(y_train_raw)\n",
        "        print(f\"Target variable 'Diagnosis' encoded. Classes: {le.classes_}\")\n",
        "        \n",
        "        X_train_scaled_np = scaler.fit_transform(X_train)\n",
        "        X_train_scaled = pd.DataFrame(X_train_scaled_np, columns=FEATURE_COLUMNS, index=X_train.index)\n",
        "        print(\"Training features scaled.\")\n",
        "    else:\n",
        "        print(\"Training data is empty after NaN handling. Cannot proceed with training.\")\n",
        "else:\n",
        "    print(\"\\nTraining features dataframe is empty. Cannot preprocess.\")\n",
        "\n",
        "\n",
        "if not df_test_features.empty and not X_train_scaled.empty: # Ensure training preprocessing happened\n",
        "    print(\"\\n--- Preprocessing Testing Data ---\")\n",
        "    df_test_clean = df_test_features.copy()\n",
        "    initial_test_rows = df_test_clean.shape[0]\n",
        "\n",
        "    nan_rows_test_count = df_test_clean[FEATURE_COLUMNS].isnull().any(axis=1).sum()\n",
        "    if nan_rows_test_count > 0:\n",
        "        if NAN_HANDLING_STRATEGY == 'drop':\n",
        "            print(f\"Strategy: Dropping {nan_rows_test_count} testing rows with NaNs in features.\")\n",
        "            df_test_clean.dropna(subset=FEATURE_COLUMNS, inplace=True)\n",
        "        elif NAN_HANDLING_STRATEGY == 'impute_mean':\n",
        "            print(f\"Strategy: Imputing NaNs in testing features using imputer fitted on training data.\")\n",
        "            try:\n",
        "                df_test_clean[FEATURE_COLUMNS] = imputer.transform(df_test_clean[FEATURE_COLUMNS])\n",
        "            except NotFittedError:\n",
        "                print(\"Error: Imputer not fitted (likely no NaNs in training or impute strategy not used). Dropping test NaNs.\")\n",
        "                df_test_clean.dropna(subset=FEATURE_COLUMNS, inplace=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error imputing test NaNs: {e}. Dropping test NaNs.\")\n",
        "                df_test_clean.dropna(subset=FEATURE_COLUMNS, inplace=True)\n",
        "    print(f\"Testing rows after NaN handling: {df_test_clean.shape[0]} (out of {initial_test_rows})\")\n",
        "\n",
        "    if not df_test_clean.empty:\n",
        "        X_test = df_test_clean[FEATURE_COLUMNS]\n",
        "        y_test_raw = df_test_clean['Diagnosis']\n",
        "        try:\n",
        "            y_test = le.transform(y_test_raw) # Use already fitted LabelEncoder\n",
        "            print(\"Test target encoded.\")\n",
        "        except ValueError as e: \n",
        "            print(f\"Warning: Error encoding test labels (some labels might be new or missing): {e}.\")\n",
        "            # Handle new labels - for now, we might just assign a placeholder or filter these rows\n",
        "            # A robust way is to ensure all labels in test are also in train, or handle them gracefully.\n",
        "            # For simplicity here, we'll try to proceed but evaluation might be tricky.\n",
        "            y_test = np.array([-1] * len(y_test_raw)) # Placeholder for unencodeable labels\n",
        "        \n",
        "        try:\n",
        "            X_test_scaled_np = scaler.transform(X_test) # Use already fitted Scaler\n",
        "            X_test_scaled = pd.DataFrame(X_test_scaled_np, columns=FEATURE_COLUMNS, index=X_test.index)\n",
        "            print(\"Testing features scaled.\")\n",
        "        except NotFittedError:\n",
        "            print(\"Error: Scaler not fitted (training data might have been empty). Cannot scale test data.\")\n",
        "            X_test_scaled = pd.DataFrame() # Ensure it's empty\n",
        "        except Exception as e:\n",
        "            print(f\"Error scaling test data: {e}\")\n",
        "            X_test_scaled = pd.DataFrame()\n",
        "    else:\n",
        "        print(\"Testing data is empty after NaN handling.\")\n",
        "elif df_test_features.empty:\n",
        "    print(\"\\nTesting features dataframe is empty. Cannot preprocess.\")\n",
        "else: # X_train_scaled is empty\n",
        "    print(\"\\nTraining data was not processed. Skipping test data preprocessing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84fe01bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [17] (Example Number) - MODEL TRAINING\n",
        "models = {} \n",
        "model_accuracies = {}\n",
        "model_cms = {}\n",
        "model_roc_aucs = {}\n",
        "\n",
        "if not X_train_scaled.empty and y_train.size > 0:\n",
        "    print(\"\\\\n--- Training All Models ---\")\n",
        "\n",
        "    # 1. Support Vector Classification (SVC)\n",
        "    print(\"\\\\nTraining Support Vector Classifier...\")\n",
        "    svc_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=RANDOM_STATE, class_weight='balanced')\n",
        "    try:\n",
        "        svc_model.fit(X_train_scaled, y_train)\n",
        "        models['SVC'] = svc_model\n",
        "        print(\"SVC Model Trained.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error training SVC: {e}\")\n",
        "        models['SVC'] = None\n",
        "\n",
        "    # 2. XGBoost Classifier\n",
        "    print(\"\\\\nTraining XGBoost Classifier...\")\n",
        "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', \n",
        "                                  use_label_encoder=False, \n",
        "                                  random_state=RANDOM_STATE) \n",
        "    try:\n",
        "        xgb_model.fit(X_train_scaled, y_train)\n",
        "        models['XGBoost'] = xgb_model\n",
        "        print(\"XGBoost Model Trained.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error training XGBoost: {e}\")\n",
        "        models['XGBoost'] = None\n",
        "\n",
        "    # 3. Random Forest Classifier\n",
        "    print(\"\\\\nTraining Random Forest Classifier...\")\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, class_weight='balanced')\n",
        "    try:\n",
        "        rf_model.fit(X_train_scaled, y_train)\n",
        "        models['RandomForest'] = rf_model\n",
        "        print(\"Random Forest Model Trained.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error training Random Forest: {e}\")\n",
        "        models['RandomForest'] = None\n",
        "\n",
        "    # 4. Neural Network Classifier (MLP)\n",
        "    print(\"\\\\nTraining Neural Network (MLP) Classifier...\")\n",
        "    mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=RANDOM_STATE, \n",
        "                              early_stopping=True, n_iter_no_change=10) \n",
        "    try:\n",
        "        mlp_model.fit(X_train_scaled, y_train)\n",
        "        models['MLP'] = mlp_model\n",
        "        print(\"MLP Model Trained.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error training MLP: {e}\")\n",
        "        models['MLP'] = None\n",
        "        \n",
        "    # 5. Logistic Regression\n",
        "    print(\"\\\\nTraining Logistic Regression Classifier...\")\n",
        "    lr_model = LogisticRegression(solver='liblinear', random_state=RANDOM_STATE, class_weight='balanced', max_iter=200) # Added max_iter\n",
        "    try:\n",
        "        lr_model.fit(X_train_scaled, y_train)\n",
        "        models['LogisticRegression'] = lr_model\n",
        "        print(\"Logistic Regression Model Trained.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error training Logistic Regression: {e}\")\n",
        "        models['LogisticRegression'] = None\n",
        "else:\n",
        "    print(\"\\\\nSkipping model training due to lack of processed training data or labels.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d52329",
      "metadata": {},
      "source": [
        "# 10. Model Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8abf99e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model_name, model, X_test_data, y_true_encoded, le_encoder):\n",
        "    print(f\"\\\\n--- Evaluation Metrics for: {model_name} on Test Set ---\")\n",
        "    if model is None: \n",
        "        print(\"Model was not trained or training failed.\"); return 0.0, None, None\n",
        "    if X_test_data.empty: \n",
        "        print(\"Test data is empty.\"); return 0.0, None, None\n",
        "    # Ensure y_true_encoded is not empty and doesn't contain only -1 (placeholder for encoding errors)\n",
        "    if len(y_true_encoded) == 0 or (isinstance(y_true_encoded, np.ndarray) and np.all(y_true_encoded == -1)): \n",
        "        print(\"Test labels are invalid or empty.\"); return 0.0, None, None\n",
        "    \n",
        "    y_pred, y_prob = None, None\n",
        "    try:\n",
        "        y_pred = model.predict(X_test_data)\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "             y_prob = model.predict_proba(X_test_data)[:, 1] \n",
        "        else:\n",
        "            print(f\"Model {model_name} does not support predict_proba, ROC AUC cannot be calculated.\")\n",
        "    except NotFittedError:\n",
        "        print(f\"Error: Model {model_name} is not fitted. Cannot make predictions.\")\n",
        "        return 0.0, None, None\n",
        "    except Exception as e: \n",
        "        print(f\"Error during prediction for {model_name}: {e}\"); return 0.0, None, None\n",
        "\n",
        "    accuracy = accuracy_score(y_true_encoded, y_pred)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\\\nClassification Report:\")\n",
        "    try:\n",
        "        present_labels_in_true_or_pred = np.unique(np.concatenate((y_true_encoded, y_pred)))\n",
        "        # Filter out -1 if it was used as a placeholder for unencodeable labels\n",
        "        present_labels = [label for label in present_labels_in_true_or_pred if label != -1]\n",
        "\n",
        "        if not present_labels: # If only -1 was present or became present\n",
        "            print(\"No valid labels found for classification report after filtering.\")\n",
        "            print(classification_report(y_true_encoded, y_pred, zero_division=0)) # Fallback\n",
        "        else:\n",
        "            present_target_names = le_encoder.inverse_transform(present_labels)\n",
        "            print(classification_report(y_true_encoded, y_pred, labels=present_labels, target_names=present_target_names, zero_division=0))\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate full classification report for {model_name}: {e}\")\n",
        "        print(classification_report(y_true_encoded, y_pred, zero_division=0))\n",
        "\n",
        "    print(\"\\\\nConfusion Matrix:\")\n",
        "    # Ensure labels for confusion matrix are only those present in the encoder\n",
        "    cm_labels = [l for l in np.unique(y_true_encoded) if l != -1] # Use only valid labels from y_true for CM\n",
        "    if not cm_labels: # If y_true only contained -1 or was empty\n",
        "        cm_labels_for_sklearn = le_encoder.transform(le_encoder.classes_) # Default to all known classes\n",
        "    else:\n",
        "        cm_labels_for_sklearn = cm_labels\n",
        "\n",
        "    if not list(cm_labels_for_sklearn): # If still no valid labels\n",
        "        print(\"No valid labels to display confusion matrix.\")\n",
        "        cm = None\n",
        "    else:\n",
        "        cm = confusion_matrix(y_true_encoded, y_pred, labels=cm_labels_for_sklearn)\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        # Use le.classes_ for tick labels as they represent the full potential set\n",
        "        tick_labels = le_encoder.inverse_transform(cm_labels_for_sklearn) if list(cm_labels_for_sklearn) else le_encoder.classes_\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                    xticklabels=tick_labels, yticklabels=tick_labels)\n",
        "        plt.xlabel('Predicted Label'); plt.ylabel('True Label'); plt.title(f'Confusion Matrix - {model_name}'); plt.show()\n",
        "\n",
        "    roc_auc = None\n",
        "    # Ensure there are at least two unique, valid classes in y_true_encoded for ROC AUC\n",
        "    valid_y_true_for_roc = y_true_encoded[y_true_encoded != -1]\n",
        "    if y_prob is not None and len(np.unique(valid_y_true_for_roc)) > 1: \n",
        "        fpr, tpr, _ = roc_curve(valid_y_true_for_roc, y_prob[y_true_encoded != -1] if len(y_prob) == len(y_true_encoded) else y_prob) # Ensure y_prob aligns\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        print(f\"\\\\nROC AUC Score: {roc_auc:.4f}\")\n",
        "        plt.figure(figsize=(7, 5))\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--'); plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title(f'ROC - {model_name}'); plt.legend(loc=\"lower right\"); plt.show()\n",
        "    elif y_prob is None:\n",
        "        print(f\"ROC AUC cannot be calculated for {model_name} as y_prob is not available.\")\n",
        "    else: # Only one class in valid_y_true_for_roc\n",
        "        print(f\"ROC AUC cannot be calculated for {model_name} because there is only one class present in valid y_true labels.\")\n",
        "            \n",
        "    return accuracy, cm, roc_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d501b620",
      "metadata": {},
      "source": [
        "# 10. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0694666",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [18] (Example Number) - MODEL EVALUATION\n",
        "print(\"\\\\n\\\\n=== Evaluating All Models on Test Set ===\")\n",
        "\n",
        "if not X_test_scaled.empty and y_test.size > 0 and np.any(y_test != -1): # Check if there are any valid test labels\n",
        "    for model_name, model_instance in models.items():\n",
        "        if model_instance is not None:\n",
        "            print(f\"\\\\n--- Evaluating {model_name} ---\")\n",
        "            # THE CALL TO evaluate_model HAPPENS HERE\n",
        "            acc, cm, roc = evaluate_model(model_name, model_instance, X_test_scaled, y_test, le)\n",
        "            model_accuracies[model_name] = acc\n",
        "            model_cms[model_name] = cm\n",
        "            model_roc_aucs[model_name] = roc\n",
        "        else:\n",
        "            print(f\"\\\\nSkipping evaluation for {model_name} as it was not trained successfully.\")\n",
        "            model_accuracies[model_name] = 0.0\n",
        "            model_cms[model_name] = None\n",
        "            model_roc_aucs[model_name] = None\n",
        "elif X_test_scaled.empty or y_test.size == 0 or np.all(y_test == -1):\n",
        "    print(\"\\\\nSkipping all model evaluations: Test data or labels are empty or invalid.\")\n",
        "else: # Should not be reached if the first condition is met\n",
        "    print(\"\\\\nSkipping all model evaluations: Training data was not processed, so no models were trained.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Conclusion & Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- FINAL SUMMARY (Cell after Model Evaluation) ---\n",
        "print(\"\\\\n\\\\n--- Final Summary ---\")\n",
        "# ... (initial print statements about data processing can remain the same) ...\n",
        "if not df_all_images.empty:\n",
        "    print(f\"Processed {len(all_image_paths_info)} total images from {df_all_images['patient_slide_id'].nunique()} unique patient/slides.\")\n",
        "    if train_image_info_list: # Check if list is not empty\n",
        "        print(f\"Used {len(train_image_info_list)} images for training feature extraction (from {len(set(item['patient_slide_id'] for item in train_image_info_list))} patients/slides).\")\n",
        "    if test_image_info_list: # Check if list is not empty\n",
        "        print(f\"Used {len(test_image_info_list)} images for testing feature extraction (from {len(set(item['patient_slide_id'] for item in test_image_info_list))} patients/slides).\")\n",
        "print(\"\\n\")\n",
        "\n",
        "if not X_train_scaled.empty:\n",
        "    print(f\"Training data samples after cleaning: {X_train_scaled.shape[0]}\")\n",
        "else:\n",
        "    print(\"Training data: N/A or empty after cleaning.\")\n",
        "\n",
        "if not X_test_scaled.empty:\n",
        "    print(f\"Testing data samples after cleaning: {X_test_scaled.shape[0]}\")\n",
        "else:\n",
        "    print(\"Testing data: N/A or empty after cleaning.\")\n",
        "\n",
        "print(\"\\\\n--- Model Performance on Test Set ---\")\n",
        "if model_accuracies: # Check if any models were evaluated\n",
        "    for model_name in models.keys(): # Iterate in the order they were defined\n",
        "        if model_name in model_accuracies:\n",
        "            print(f\"Accuracy ({model_name}): {model_accuracies.get(model_name, 'N/A - Not Evaluated'):.4f}\")\n",
        "            if model_roc_aucs.get(model_name) is not None:\n",
        "                print(f\"ROC AUC ({model_name}):  {model_roc_aucs[model_name]:.4f}\")\n",
        "            else:\n",
        "                print(f\"ROC AUC ({model_name}):  N/A\")\n",
        "        else:\n",
        "            print(f\"{model_name}: Not trained or evaluated.\")\n",
        "    print(\"\\\\nReview detailed metrics (precision, recall, F1, Confusion Matrix) for each model above.\")\n",
        "else:\n",
        "    print(\"No models were successfully trained or evaluated.\")\n",
        "\n",
        "print(\"\\\\n\\\\nConsiderations & Next Steps:\")\n",
        "print(\"- Segmentation Parameters: Ensure `SEGMENTATION_CONFIG` reflects your best tuned values for the target magnification(s).\")\n",
        "print(\"- Hyperparameter Tuning: For each model type, use GridSearchCV or RandomizedSearchCV on the *training set*.\")\n",
        "print(\"- Feature Importance/Selection: Analyze which features are most predictive for the best performing model(s).\")\n",
        "print(\"- Addressing Low Precision for Benign: This requires careful analysis of misclassified malignant cases (see previous detailed advice). It might involve segmentation tweaks, feature engineering, or model-specific adjustments like changing the decision threshold.\")\n",
        "print(\"- Ensemble Methods: Consider combining predictions from your top models.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
